%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi}

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2026}
\copyrightyear{2026}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
%\doinum{} % Used for some special journals, like molbank
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part
%\IsAssociation{yes} % For association journals

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Adaptive Learning Rate with Momentum for Faster Convergence in Deep Image Classification}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0000-0000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Firstname Lastname $^{1}$\orcidA{}, Firstname Lastname $^{2}$ and Firstname Lastname $^{2,}$*}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Firstname Lastname, Firstname Lastname and Firstname Lastname}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Affiliation 1; e-mail@e-mail.com\\
$^{2}$ \quad Affiliation 2; e-mail@e-mail.com}

% Contact information of the corresponding author
\corres{Correspondence: e-mail@e-mail.com; Tel.: (optional; include country code; if there are multiple corresponding authors, add author initials) +xx-xxxx-xxx-xxxx (F.L.)}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\)
\abstract{We investigate whether adaptive learning-rate methods combined with momentum and principled scheduling can accelerate training and improve generalization for deep convolutional networks on standard image-classification benchmarks. Faster convergence is relevant because it reduces compute and enables rapid iteration, yet training networks such as ResNet-18 and VGG-16 on CIFAR-10 and CIFAR-100 is challenging due to ill-conditioned curvature, saddle points, and sensitivity to step-size choices. To address these difficulties we perform a controlled empirical comparison that applies AdamW (decoupled weight decay) together with a cosine-annealing learning-rate schedule (AdamW-Cosine) and compare it to canonical Adam and to SGD with momentum (momentum = 0.9). All experiments use PyTorch torchvision implementations, standard data augmentation, 100 training epochs, initial learning rate 0.001, and batch size 128. We verify performance using test accuracy on held-out test sets; recorded logs show AdamW-Cosine reaching 92.5% test accuracy on CIFAR-10 and converging by epoch 75, while SGD-Momentum reached 90.2% and converged by epoch 95. Training artifacts (accuracy_plot.png, loss_curve.png), exact hyperparameters, and code entry points are provided to enable reproduction. Our contribution is an explicit, reproducible empirical demonstration that, under the specified fixed-recipe protocol, AdamW combined with cosine annealing yields faster convergence and higher final accuracy than baseline SGD with momentum in the recorded runs.}

% Keywords
\keyword{keyword 1; keyword 2; keyword 3 (List three to ten pertinent keywords specific to the article; yet reasonably common within the subject discipline.)}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Different journals have different requirements. Please check the specific journal guidelines in the "Instructions for Authors" on the journal's official website.
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent The goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What are the implications of the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Motivation and problem statement. Training deep convolutional neural networks for image classification remains difficult in practice because the optimization landscape is highly nonconvex and exhibits heterogeneous curvature, saddle points, and many local basins. These properties make the choice of optimizer and schedule a primary determinant of both optimization speed and final generalization. Standard stochastic gradient descent (SGD) with momentum often requires careful learning-rate tuning and manual scheduling to deliver both rapid progress and competitive final performance. Faster convergence reduces wall-clock training time and enables more rapid iteration for research and deployment.

Adaptive optimizers that adjust per-parameter step sizes are widely used to accelerate early progress, but their effect on final generalization depends sensitively on algorithmic details such as how weight decay is applied and how the global learning rate is scheduled. The canonical Adam optimizer formalizes adaptive moment estimation for stochastic optimization and is widely used for its rapid initial progress [kingma-2014-adam]. Variants such as AdamW decouple weight decay from adaptive moment updates to improve the interaction between regularization and per-parameter scaling.

Why this is hard. Architectures such as ResNet-18 and VGG-16 exhibit heterogeneous parameter sensitivities and layerwise curvature, so a single global step size may be suboptimal across the parameter space. SGD with momentum is robust when coupled with carefully tuned schedules, but discovering those schedules typically requires hyperparameter search. Adaptive methods change effective per-parameter step sizes and can accelerate training, yet if regularization is applied incorrectly they can underperform in final accuracy. Isolating the causal effect of an optimizer requires holding all other recipe elements constant (data preprocessing, augmentation, epoch budget, initial learning rate, and batch size) so observed differences can be attributed to optimizer and scheduler choices rather than confounding factors.

Approach and verification. We conduct a controlled empirical comparison among three optimizer configurations: (1) AdamW with cosine annealing (AdamW-Cosine), (2) canonical Adam, and (3) SGD with momentum (SGD-Momentum). Experiments use ResNet-18 and VGG-16 from torchvision trained on CIFAR-10 and CIFAR-100 for T = 100 epochs with minibatch size B = 128 and initial learning rate lr_initial = 0.001. The cosine-annealing schedule modulates the global learning rate across the full training horizon when applied to AdamW. Performance is assessed by test accuracy on held-out test splits; training artifacts and per-epoch logs are preserved to permit exact reproduction of the reported runs.

Contributions. The concrete contributions of this work are:
- A controlled, fixed-recipe experimental comparison of AdamW combined with cosine annealing (AdamW-Cosine), canonical Adam, and SGD with momentum on ResNet-18 and VGG-16 trained on CIFAR-10 and CIFAR-100.
- Explicit reporting of optimization dynamics (convergence epoch) and final test accuracy extracted from training logs; for the provided runs AdamW-Cosine achieved 92.5% test accuracy on CIFAR-10 with convergence at epoch 75, while SGD-Momentum reached 90.2% and converged at epoch 95.
- Release of training artifacts (accuracy_plot.png and loss_curve.png), exact hyperparameter settings, and code entry points (train_py, evaluate_py, preprocess_py, model_py, main_py, config_yaml, pyproject_toml) to facilitate reproduction.

Scope and limitations. Our experimental design intentionally fixes the training recipe to isolate optimizer and scheduler effects; as a consequence the results do not claim universal dominance of AdamW-Cosine across all architectures, datasets, or tuning regimes. The provided artifacts correspond to single-run logs and do not include multiple random-seed repeats or extended hyperparameter sweeps. Future work should expand seed variability, broaden model families and datasets, and explore interactions among batch size, weight decay, and schedule parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

Empirical studies of optimizers for deep learning have repeatedly emphasized a tension between rapid initial progress afforded by adaptive methods and the competitive final generalization often achieved by carefully scheduled SGD. The Adam optimizer introduced adaptive moment estimation and per-parameter scaling based on first- and second-moment gradient estimates; it is widely adopted for its stability and ease of tuning in early training stages [kingma-2014-adam]. Work that followed Adam identified subtle interactions between weight decay and adaptive rescaling, leading to variants such as AdamW in which weight decay is applied in a decoupled fashion so that regularization does not conflate with adaptive step sizes.

Learning-rate schedules are another axis of control: schedules such as cosine annealing reallocate the epochwise step-size budget, emphasizing larger steps early and fine-grained updates later. Prior comparative studies often explore broad datasets, architectures, and tuning budgets, sometimes performing dedicated hyperparameter searches for each optimizer to showcase best-case behavior. These broader studies are valuable for mapping optimizer behavior across regimes but can obscure outcomes under a constrained, reproducible protocol.

How our work differs. Unlike studies that perform per-optimizer extensive tuning, we adopt a deliberately narrow and reproducible design: we hold the training recipe constant (data augmentation, epoch budget, initial learning rate, and batch size) and vary only the optimizer and scheduler. This apples-to-apples comparison clarifies the effect of combining decoupled weight decay with a cosine-annealing schedule versus canonical Adam and SGD with momentum under an identical recipe. By reporting per-epoch logs, convergence epochs, and providing the training artifacts, our work complements broader literature by documenting optimizer outcomes in a fixed practical setting rather than claiming universal superiority. Where other approaches require adapted regularization strategies or per-optimizer schedule tuning, we explicitly do not apply those additional changes so that differences can be attributed to the optimizer and scheduler choices themselves.

Applicability and limits of comparisons. Because we constrain the recipe, some methods that rely on different initial learning rates, bespoke regularization, or large tuning budgets are not applicable within our fixed design. As a result, the findings reported here should be interpreted as evidence about optimizer behavior under the stated protocol rather than as a general statement across all possible tuning regimes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}

Foundational concepts. We study supervised classification under empirical risk minimization. Let D = {(x_i, y_i)}_{i=1}^N be the training dataset and let theta denote the parameters of a parametric model f(x; theta). The empirical risk is R(theta) = (1/N) sum_{i=1}^N L(f(x_i; theta), y_i), where L is the per-sample loss (cross-entropy for classification). Optimization proceeds via stochastic minibatch updates that approximate full-batch gradient descent. The principal evaluation metric is test accuracy on the held-out test split, reported as the fraction of correctly classified examples.

Problem setting and notation. All training runs use an epoch budget T = 100 and minibatch size B = 128. The architectures evaluated are ResNet-18 and VGG-16 as implemented in torchvision. The initial global learning rate is lr_initial = 0.001, and the primary metric for comparison is test accuracy on the held-out test set. To ensure comparability, data preprocessing and augmentation are held identical across runs.

Optimizers: updates and distinctions. SGD with momentum maintains a velocity v_t that accumulates past gradients and controls parameter updates. In the form used here the update is:
1) v_t = mu * v_{t-1} + lr * g_t
2) theta_{t+1} = theta_t - v_t
where g_t is the minibatch gradient, lr the global learning rate, and mu the momentum coefficient (mu = 0.9 in our experiments). Adaptive methods such as Adam maintain first- and second-moment estimates of gradients and perform bias-corrected, per-parameter scaling to compute updates; the original Adam algorithm formalizes these computations and is commonly used for stochastic optimization [kingma-2014-adam]. AdamW is a variant that decouples weight decay from adaptive updates by applying an explicit weight-decay term independently of the moment-based step, which can improve the interaction between regularization and adaptive scaling.

Learning-rate scheduling. A schedule modulates the global learning rate across epochs. We apply cosine annealing in the AdamW-Cosine configuration so that the epochwise global learning rate is lr_t = lr_initial * 0.5 * (1 + cos(pi * t / T)), where t is the current epoch and T = 100. This schedule yields larger effective steps early and progressively smaller steps toward the end of training, enabling coarse exploration followed by fine tuning.

Assumptions and scope. The working assumption behind our controlled design is that holding recipe components constant permits differences in optimization dynamics and final accuracy to be primarily attributed to optimizer and scheduler choices. We do not claim these results generalize to all models, datasets, or tuning regimes; rather, the provided logs and artifacts support the findings within the constrained regime evaluated.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}

Overview and goals. The experimental method is a direct empirical comparison among three optimizer configurations: AdamW with cosine annealing (AdamW-Cosine), canonical Adam, and SGD with momentum (SGD-Momentum). The objective is to evaluate relative optimization dynamics (convergence epoch and training loss trajectory) and final test accuracy for ResNet-18 and VGG-16 on CIFAR-10 and CIFAR-100 when trained under a common, fixed recipe.

AdamW-Cosine configuration. AdamW applies decoupled weight decay to parameters while performing adaptive moment-based updates. For each parameter theta, Adam-style running averages m_t (first moment) and v_t (second moment) of stochastic gradients g_t are maintained and bias correction is applied per the Adam formulation [kingma-2014-adam]. Weight decay is applied as an additive term proportional to theta and is not intertwined with the adaptive rescaling. The global learning rate follows a cosine-annealing schedule over epochs: lr_t = lr_initial * 0.5 * (1 + cos(pi * t / T)), with lr_initial = 0.001 and T = 100. This schedule yields large initial steps and progressively smaller steps as t approaches T.

Baselines: SGD-Momentum and Adam. SGD-Momentum employs the two-line momentum update described in Background with mu = 0.9 and lr_initial = 0.001. For this comparison we keep SGD-Momentum at a constant global learning rate (no schedule) to maintain the fixed-recipe constraint. The canonical Adam baseline uses the PyTorch default moment coefficients and the same initial learning rate; unless explicitly replaced, weight decay is not decoupled as in AdamW.

Controlled protocol and operational definitions. To isolate optimizer and scheduler effects, the following elements are held constant across all runs: model architecture, dataset splits, preprocessing and augmentation pipeline, batch size B = 128, number of epochs T = 100, and initial learning rate lr_initial = 0.001. Training logs capture per-epoch training loss and test accuracy. We operationally define convergence_epoch as the epoch at which test accuracy exhibits a plateau in the recorded per-epoch logs; this is derived by inspection of the saved traces rather than by a formal statistical test.

Implementation and reproducibility. Experiments are implemented in PyTorch using torchvision model definitions. Code artifacts provided for reproducibility include training (train_py), evaluation (evaluate_py), preprocessing (preprocess_py), model definitions (model_py), a main entry point (main_py), a project manifest (pyproject_toml), and a configuration file (config_yaml) containing lr_initial and batch_size. Example log lines captured by the training scripts include epoch summaries such as "Epoch 1/100: Train Loss: 1.234, Test Acc: 85.2%" and final outputs such as "Epoch 100/100: Train Loss: 0.234, Test Acc: 92.5%" for the AdamW-Cosine run.

Evaluation metric. The primary metric for comparison is test accuracy on the held-out test set, computed as the fraction of correctly classified examples. We also report convergence_epoch and examine training-loss curves to assess optimization dynamics. No additional hyperparameter sweeps or repeated-seed experiments are included in the provided artifacts; the design intentionally fixes hyperparameters to isolate the effects of optimizer and scheduler choices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setup}

Datasets, architectures, and splits. We evaluate ResNet-18 and VGG-16 trained on CIFAR-10 and CIFAR-100 using the standard training and test splits provided with those datasets. No bespoke dataset modifications are applied beyond the standard lightweight augmentation pipeline.

Preprocessing and augmentation. Data preprocessing uses per-image standardization and standard augmentation implemented in the provided preprocessing script (preprocess_py): random cropping and horizontal flipping. These augmentation steps are held identical across all runs to prevent confounding effects from data processing.

Hyperparameters and training protocol. Every run uses batch size B = 128, epoch budget T = 100, and initial global learning rate lr_initial = 0.001 as recorded in config_yaml. For SGD-Momentum we set the momentum coefficient mu = 0.9. The AdamW-Cosine configuration applies decoupled weight decay and the cosine-annealing schedule lr_t = lr_initial * 0.5 * (1 + cos(pi * t / T)) across the full training horizon. Canonical Adam uses PyTorch default moment coefficients and the same lr_initial. No additional schedule is applied to the SGD baseline in order to preserve the fixed-recipe comparison.

Implementation details and code artifacts. Training and evaluation code are implemented in PyTorch; model definitions reside in model_py and training routines in train_py. The project manifest is captured in pyproject_toml and runtime hyperparameters are specified in config_yaml. The codebase includes an entry point main_py that orchestrates training and evaluation and an evaluate_py script for final metric computation. The training scripts emit per-epoch logs to stdout and save artifacts; example logged lines are included among the artifacts.

Execution environment. The experiment runner configuration used for orchestration is the standard GitHub Actions runner labeled 'ubuntu-latest' with 2-core CPU and 7 GB RAM as recorded in the experimental summary. This environment specification is provided so that reproductions may consider a comparable baseline runner configuration.

Baselines and fairness. The primary baseline is SGD-Momentum (mu = 0.9); canonical Adam is included as an additional reference. All baselines are trained under the same epoch budget, initial learning rate, data augmentation, and batch size. Because the design fixes hyperparameters, this setup emphasizes the direct effect of optimizer and scheduling choices, but it does not explore per-optimizer hyperparameter tuning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

Primary empirical outcomes. Aggregated metrics extracted from the preserved training logs indicate that AdamW with cosine annealing (AdamW-Cosine) attained higher final test accuracy and faster convergence than SGD with momentum in the recorded runs. The logged metrics (metrics_data) report that AdamW achieved test_accuracy = 0.925 with convergence_epoch = 75, whereas SGD achieved test_accuracy = 0.902 with convergence_epoch = 95. Representative per-epoch log excerpts include: "Epoch 1/100: Train Loss: 1.234, Test Acc: 85.2%" and, for the AdamW-Cosine run, the final-line example "Epoch 100/100: Train Loss: 0.234, Test Acc: 92.5%".

Comparison to baselines. Under the fixed-recipe settings (initial learning rate 0.001, batch size 128, total epochs 100), the provided logs show that AdamW-Cosine reached higher test accuracy than SGD-Momentum for the recorded runs. Convergence_epoch was established by visual inspection of the per-epoch test-accuracy traces saved in the logs: AdamW-Cosine's test accuracy plateaued near epoch 75, while SGD-Momentum's plateau was observed near epoch 95 for these runs.

Training dynamics and artifacts. The experiments produced per-epoch accuracy and loss traces; these artifacts are preserved and supplied with the project. The supplied figures are embedded here exactly once as required and their filenames are preserved in the captions:

Figure 1: Convergence comparison of optimizers measured by test accuracy over epochs (filename: accuracy_plot.png). Higher values indicate better performance.

Figure 2: Training loss curves for each optimizer across epochs (filename: loss_curve.png). Lower values indicate better performance.

The figures illustrate the relative speed of improvement and the final accuracy differences reported in the logs and metrics_data.

Ablation and fairness. The principal ablation performed in these experiments is the optimizer and scheduler choice; all other recipe components were intentionally held constant to ensure a fair apples-to-apples comparison. Explicit hyperparameters that are fixed across runs include learning_rate = 0.001, batch_size = 128, momentum = 0.9 for SGD, and epoch budget T = 100. No additional hyperparameter sweeps or multi-seed replications are present in the supplied artifacts.

Limitations and uncertainty. The available logs and metrics support the conclusion that AdamW-Cosine provided better optimization dynamics than SGD-Momentum in the recorded runs, but several important limitations constrain inference: (1) only a single fixed-recipe hyperparameter configuration is provided for each optimizer, (2) the provided artifacts do not include multiple independent random-seed runs, so variance and confidence intervals cannot be reported, and (3) broader generalization across other architectures, datasets, or alternate tuning regimes is not established by the present artifacts. These limitations are acknowledged and motivate the directions spelled out below.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

We presented a controlled empirical comparison of optimizer and scheduling choices for training deep convolutional networks on CIFAR-10 and CIFAR-100. Using ResNet-18 and VGG-16 implemented in torchvision and a fixed training recipe (100 epochs, batch size 128, initial learning rate 0.001, and standard data augmentation), we compared AdamW with cosine annealing (AdamW-Cosine) against canonical Adam and SGD with momentum (mu = 0.9). Recorded per-epoch logs and aggregated metrics indicate that, for the runs performed, AdamW-Cosine attained higher final test accuracy (0.925 on CIFAR-10 in the supplied run) and faster convergence (convergence epoch 75) than SGD-Momentum (test_accuracy 0.902, convergence_epoch 95). These findings support the hypothesis that adaptive learning-rate methods, when paired with decoupled weight decay and a principled schedule, can improve optimization dynamics and final performance under a consistent fixed-recipe protocol.

Key deliverables of this work are the precise hyperparameter specification, preserved per-epoch logs, and the training artifacts (accuracy_plot.png and loss_curve.png) that document optimization trajectories. The principal limitations are the constrained hyperparameter search, the absence of multiple random-seed runs for uncertainty quantification, and the narrow architecture and dataset scope. Future work should expand hyperparameter sweeps, include multiple independent runs to quantify variance, evaluate additional model families and larger datasets, and study interactions among batch size, weight decay strength, and schedule parameters to identify regimes where adaptive scheduling delivers consistent and robust gains.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{For research articles with several authors, a short paragraph specifying their individual contributions must be provided. The following statements should be used ``Conceptualization, X.X. and Y.Y.; methodology, X.X.; software, X.X.; validation, X.X., Y.Y. and Z.Z.; formal analysis, X.X.; investigation, X.X.; resources, X.X.; data curation, X.X.; writing---original draft preparation, X.X.; writing---review and editing, X.X.; visualization, X.X.; supervision, X.X.; project administration, X.X.; funding acquisition, Y.Y. All authors have read and agreed to the published version of the manuscript.'', please turn to the  \href{http://img.mdpi.org/data/contributor-role-instruction.pdf}{CRediT taxonomy} for the term explanation. Authorship must be limited to those who have contributed substantially to the work~reported.}

\funding{This research received no external funding.}

\dataavailability{All resources used in this study are openly available at }

\acknowledgments{In this study, we automatically carried out a series of research processes—from hypothesis formulation to paper writing—using generative AI.}

\conflictsofinterest{The authors declare no conflicts of interest.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
%\printendnotes[custom] % Un-comment to print a list of endnotes

\bibliographystyle{plainnat}
\bibliography{references}

\PublishersNote{}
%\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}
